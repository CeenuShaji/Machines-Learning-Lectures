{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8 - Conjugate Prior Relationship; Online Update of Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:blue\">Maximum Likelihood Estimation (MLE)</span></h2>\n",
    "<center>(Frequentist approach)</center>\n",
    "\n",
    "$$\\arg_{\\mathbf{w}} \\max P(\\mathbf{t}|\\mathbf{w})$$\n",
    "\n",
    "In **Maximum Likelihood Estimation** we *find the set of parameters* that **maximize** the data likelihood $P(\\mathbf{t}|\\mathbf{w})$. We find the *optimal* set of parameters under some assumed distribution such that the data is most likely.\n",
    "\n",
    "* MLE focuses on maximizing the data likelihood, which *usually* provides a pretty good estimate\n",
    "\n",
    "* A common trick to maximize the data likelihood is to maximize the log likelihood\n",
    "\n",
    "* MLE is purely data driven \n",
    "\n",
    "* MLE works best when we have lots and lots of data\n",
    "\n",
    "* MLE will likely overfit when we have small amounts of data or, at least, becomes unreliable\n",
    "\n",
    "* It estimates relative frequency for our model parameters. Therefore it needs incredibly large amounts of data (infinite!) to estimate the true likelihood parameters\n",
    "    * This is a problem when we want to make inferences and/or predictions outside the range of what the training data has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:orange\">Maximum A Posteriori (MAP)</span></h2>\n",
    "<center>(Bayesian approach)</center>\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}} \\max P(\\mathbf{t}|\\mathbf{w})P(\\mathbf{w}) \\\\ \n",
    "& \\propto \\arg_{\\mathbf{w}} \\max P(\\mathbf{w}|\\mathbf{t})\n",
    "\\end{align}\n",
    "\n",
    "In **Maximum A Posteriori** we *find the set of parameters* that **maximize** the the posterior probability $P(\\mathbf{w}|\\mathbf{t})$. We find the *optimal* set of parameters under some assumed distribution such that the parameters are most likely to have been drawn off of.\n",
    "\n",
    "* MAP focuses on maximizing the posterior probability - data  likelihood with a prior\n",
    "\n",
    "* A common trick to maximize the posterior probability is to maximize the log likelihood\n",
    "\n",
    "* MAP is data driven \n",
    "\n",
    "* MAP is mostly driven by the prior beliefs\n",
    "\n",
    "* MAP works great with small amounts of data *if* our prior was chosen well\n",
    "\n",
    "* We need to assume and select a distribution for our prior beliefs\n",
    "    * A wrong choice of prior distribution can impact negatively our model estimation\n",
    "    \n",
    "* When we have lots and lots of data, the data likelihood will take over and the posterior will depend less and less on the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "From last class, we illustrated the example where our input dataset $\\{x_i\\}_{i=1}^N$ is binary, where $x_i=\\{0,1\\}, \\forall i$. \n",
    "\n",
    "For each data sample $x_i$, we modeled its data likelihood as the Bernoulli distribution with parameter $\\mu$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "x_i \\sim \\text{Bernoulli}(\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "Furthermore, we assume that each sample is independent of each other. Hence our dataset is a collection of i.i.d. samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE Approach**\n",
    "\n",
    "The MLE finds the solution for the parameter $\\mu$ by maximizing the data likelihood. \n",
    "\n",
    "The observed data likelihood can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{\\text{MLE}}^0 &= \\prod_{i=1}^N \\mu^{x_i}(1-\\mu)^{1-x_i}\n",
    "\\end{align*}\n",
    "\n",
    "The **MLE** solution for the unknown parameter to be:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{\\text{MLE}} = \\frac{\\sum_{i=1}^N x_i}{N}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP Approach**\n",
    "\n",
    "The MAP finds the solution for the parameter $\\mu$ by maximizing the data likelihood times a prior probability on the parameter/s. \n",
    "\n",
    "Considering the Beta distribution as the prior probability for the parameter, the observed posterior can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{\\text{MAP}}^0 &= \\left(\\prod_{i=1}^N \\mu^{x_i}(1-\\mu)^{1-x_i} \\right)\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}\\\\\n",
    "&\\propto \\left(\\prod_{i=1}^N \\mu^{x_i}(1-\\mu)^{1-x_i} \\right)\\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}\\\\\n",
    "&= \\mu^{\\sum_{i=1}^N x_i+\\alpha-1} (1-\\mu)^{N-\\sum_{i=1}^N x_i+\\beta-1}\n",
    "\\end{align*}\n",
    "\n",
    "The **MAP** solution for the unknown parameter to be:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{\\text{MAP}} = \\frac{\\sum_{i=1}^N x_i+\\alpha-1}{N + \\alpha + \\beta -2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 30\n",
    "b = 30\n",
    "Beta = stats.beta(a,b)\n",
    "x = np.linspace(0,1,1000)\n",
    "\n",
    "plt.plot(x, Beta.pdf(x), label='Beta Distribution')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Probability of Heads\\n $\\mu$',fontsize=15)\n",
    "plt.ylabel('Prior Density Function\\n $P(\\mu)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "\n",
    "Nflips = 10\n",
    "\n",
    "Outcomes = []\n",
    "for i in range(Nflips):\n",
    "    Outcomes += [stats.bernoulli(trueMU).rvs(1)[0]]\n",
    "    print(Outcomes)\n",
    "    print('MLE (Frequentist, data-driven): Probability of Heads = ', np.sum(Outcomes)/len(Outcomes))\n",
    "    print('MAP (Bayesian, uses prior): Probability of Heads = ', (np.sum(Outcomes)+a-1)/(len(Outcomes)+a+b-2))\n",
    "    input('Press enter to flip the coin again...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True value of the unknown parameter\n",
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "\n",
    "# Prior Initial Parameters\n",
    "a=30 # alpha\n",
    "b=30 # beta\n",
    "\n",
    "# Sampling Training Data\n",
    "Nflips = 100\n",
    "Outcomes = stats.bernoulli(trueMU).rvs(Nflips)\n",
    "\n",
    "# Computing MLE and MAP estimates as data is being collected\n",
    "mu_MLE = []\n",
    "mu_MAP = []\n",
    "for i in range(1,Nflips+1):\n",
    "    mu_MLE += [np.sum(Outcomes[:i])/len(Outcomes[:i])]\n",
    "    mu_MAP += [(np.sum(Outcomes[:i])+a-1)/(len(Outcomes[:i])+a+b-2)]\n",
    "\n",
    "# Plotting estimates\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,Nflips+1), mu_MLE, '-or', label='MLE')\n",
    "plt.plot(range(1,Nflips+1), mu_MAP, '-*b', label='MAP')\n",
    "plt.plot(range(1,Nflips+1), [trueMU]*Nflips, '--g', label='True $\\mu$')\n",
    "plt.xlabel('Data points',size=15)\n",
    "plt.ylabel('MLE/MAP Estimations for $\\mu$',size=15)\n",
    "plt.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could we update the prior as we receive data?\n",
    "\n",
    "* What if we could fine-tune the prior probability's parameters ($\\alpha$ and $\\beta$, in this example) as we see more data?\n",
    "\n",
    "* Could we use the posterior probability to update the prior probability's parameters? That is, to select new values for $\\alpha$ and $\\beta$ using a data informative prior?\n",
    "\n",
    "* What cases would make this possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Priors\n",
    "\n",
    "Two distributions have a **conjugate prior** relationship when the form of the posterior is the same as the form of the prior.\n",
    "\n",
    "* Do the **Bernoulli-Beta** distributions have a conjugate prior relationship? Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the posterior probability was defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mu|\\mathbf{x}) &\\propto \\mu^{\\sum_{i=1}^N x_i+\\alpha-1} (1-\\mu)^{N-\\sum_{i=1}^N x_i+\\beta-1}\n",
    "\\end{align*}\n",
    "\n",
    "and, the prior probability (Beta distribution) is:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mu|\\alpha, \\beta) &\\propto \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}\n",
    "\\end{align*}\n",
    "\n",
    "(In both cases, the constant term was disregarded because it will not affect the solution for $\\mu$ during optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **posterior** and the **prior** probability have the same shape, hence they have a **<font color='blue'>conjugate prior</font>** relationship. Moreover, see that the parameters $\\alpha$ and $\\beta$ are now mapped to:\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha^{(t+1)} &\\leftarrow \\alpha^{(t)} + \\sum_{i=1}^N x_i\\\\\n",
    "\\beta^{(t+1)} &\\leftarrow \\beta^{(t)} + N - \\sum_{i=1}^N x_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an online model estimation scenario, where the posterior has the same form as the prior, we can use the posterior as our new prior. This new prior is now data informative and will update it's parameters based on (1) our initial choice, and (2) the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Code for Online Update of the Prior\n",
    "\n",
    "1. Iteration $t=0$\n",
    "2. Initialize the parameters of the prior probability, $\\alpha^{(t)}$ and $\\beta^{(t)}$\n",
    "3. As data comes in:\n",
    "    1. Compute the posterior probability, $\\mathcal{L}_{\\text{MAP}}^{(t)} = P(\\mathbf{x}|\\mu)P(\\mu|\\alpha^{(t)},\\beta^{(t)})$\n",
    "    2. Make an estimate for the parameter, $\\mu_{\\text{MAP}}^{(t)}$\n",
    "    3. Update parameters of prior probability:\n",
    "    \\begin{align*}\n",
    "    \\alpha^{(t+1)} &\\leftarrow \\alpha^{(t)} + \\sum_{i=1}^N x_i\\\\\n",
    "    \\beta^{(t+1)} &\\leftarrow \\beta^{(t)} + N - \\sum_{i=1}^N x_i\n",
    "    \\end{align*}\n",
    "    4. $t \\leftarrow t + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can implement this in code for this working example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 30\n",
    "b = 1\n",
    "Beta = stats.beta(a,b)\n",
    "x = np.linspace(0,1,1000)\n",
    "\n",
    "plt.plot(x, Beta.pdf(x), label='Beta Distribution')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Probability of Heads\\n $\\mu$',fontsize=15)\n",
    "plt.ylabel('Prior Density Function\\n $P(\\mu)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True value of the unknown parameter\n",
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "\n",
    "# Prior Probability parameters\n",
    "a = 30; a_init = a\n",
    "b = 1;  b_init = b\n",
    "\n",
    "# Sampling Training Data\n",
    "Nflips = 100\n",
    "Outcomes = stats.bernoulli(trueMU).rvs(Nflips)\n",
    "\n",
    "# Computing MLE and MAP estimates as data is being collected\n",
    "mu_MLE = []\n",
    "mu_MAP = []\n",
    "mu_MAP_update = []\n",
    "for i in range(1,Nflips+1):\n",
    "    mu_MLE += [np.sum(Outcomes[:i])/len(Outcomes[:i])]\n",
    "    mu_MAP += [(np.sum(Outcomes[:i])+a_init-1)/(len(Outcomes[:i])+a_init+b_init-2)]\n",
    "    mu_MAP_update += [(np.sum(Outcomes[:i])+a-1)/(len(Outcomes[:i])+a+b-2)]\n",
    "    a += np.sum(Outcomes[:i])\n",
    "    b += len(Outcomes[:i]) - np.sum(Outcomes[:i])\n",
    "\n",
    "# Plotting estimates\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,Nflips+1), mu_MLE, '-or', label='MLE')\n",
    "plt.plot(range(1,Nflips+1), mu_MAP, '-*b', label='MAP (no update)')\n",
    "plt.plot(range(1,Nflips+1), mu_MAP_update, '-^m', label='MAP (online update)')\n",
    "plt.plot(range(1,Nflips+1), [trueMU]*Nflips, '--g', label='True $\\mu$')\n",
    "plt.xlabel('Data points',size=15)\n",
    "plt.ylabel('MLE/MAP Estimations for $\\mu$',size=15)\n",
    "plt.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now see the effects on the posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# True value of the unknown parameter\n",
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "\n",
    "# Prior Probability parameters\n",
    "a = 30; a_init = a\n",
    "b = 1;  b_init = b\n",
    "\n",
    "# Plotting data\n",
    "x = np.linspace(-0.1,1.1,100)\n",
    "xr = range(-1,3)\n",
    "\n",
    "# Prior probability, Beta(a,b)\n",
    "plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu$)')\n",
    "plt.title('Initial Prior: alpha='+str(a)+', beta='+str(b),size=15)\n",
    "plt.show();\n",
    "\n",
    "print('Alpha = ', a)\n",
    "print('Beta = ', b)\n",
    "\n",
    "Nsamples = [1,2,5,10,50,100,150,200,500]\n",
    "Outcomes = stats.bernoulli(trueMU).rvs(size=Nsamples[-1])\n",
    "for i in range(len(Nsamples)):\n",
    "    Data = Outcomes[:Nsamples[i]]\n",
    "    \n",
    "    # Outcomes will have 1's or 0's (1 - Heads, 0 - Tails) \n",
    "    estimate_mu = (np.sum(Data)+a-1)/(len(Data)+a+b-2)\n",
    "    \n",
    "    # Data Likelihood:\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.stem(xr, stats.bernoulli(np.sum(Data)/len(Data)).pmf(xr))\n",
    "    plt.xlabel('$x$'); plt.ylabel('P(X|$\\mu$)'); \n",
    "    plt.title('Data Likelihood, '+str(Nsamples[i])+' samples')\n",
    "    \n",
    "    # Posterior/Prior:\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "    plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu|$alpha, beta)'); \n",
    "    plt.title('Posterior/Prior: alpha='+str(a)+', beta='+str(b))\n",
    "    plt.show()\n",
    "    \n",
    "    # Update Prior distribution\n",
    "    a += np.sum(Data)\n",
    "    b += len(Data)-np.sum(Data)\n",
    "    \n",
    "    # Print estimate for mu\n",
    "    print('Number of samples: ', len(Data))\n",
    "    print('Data: ',Data)\n",
    "    print('MAP estimate mu = ', estimate_mu)\n",
    "    print('New alpha = ', a)\n",
    "    print('New beta = ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many conjugate prior relationships, some examples include: \n",
    "1. Bernoulli-Beta,\n",
    "2. Gaussian-Gaussian, \n",
    "3. Gaussian-Inverse Wishart,\n",
    "4. Multinomial-Dirichlet,\n",
    "5. and others.\n",
    "\n",
    "The [table of conjugate distributions](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions) is very useful for selecting the prior probability in order to have a conjugate prior relationship for cases when the data likelihood is discrete (such as Bernoulli) or continuous (such as Gaussian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Gaussian Distribution\n",
    "\n",
    "(Read [section 2.3 \"The Gaussian Distribution\"](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) from the Bishop textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Gaussian\n",
    "\n",
    "The Gaussian distribution is a widely used probabilistic model for the probability density function (pdf) of continuous random variables. \n",
    "\n",
    "The Gaussian distribution can model both univariate (1-D) or multivariate (multi-dimensional) samples.\n",
    "\n",
    "In the **univariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, or, $X\\sim N(\\mu,\\sigma^2)$.\n",
    "\n",
    "* We can define the **precision** parameter $\\beta$ as the inverse of the variance, that is, $\\beta=\\frac{1}{\\sigma^2}$.\n",
    "\n",
    "* A Gaussian distribution is called **Normal** when the mean is $\\mu=0$ and variance is $\\sigma^2=1$, $X\\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1=stats.norm(0,1) # G(0,1^2)\n",
    "\n",
    "G2=stats.norm(10,3) #G(10,3^2)\n",
    "\n",
    "G3=stats.norm(-5,0.3) #G(-5, 0.3^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "x=np.linspace(-8,18,1000)\n",
    "plt.plot(x,G1.pdf(x),label='Gaussian(0,1)')\n",
    "plt.plot(x,G2.pdf(x),'--',label='Gaussian(10,9)')\n",
    "plt.plot(x,G3.pdf(x),'-.',label='Gaussian(-5,0.09)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Probability Density Function \\n $f_X(x)$',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=G1.rvs(size=100)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "x=np.linspace(-5,5,100)\n",
    "plt.hist(samples,density=True, label='Histogram for\\n 100 samples')\n",
    "plt.plot(x, G1.pdf(x), label='pdf for G(0,1)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Density',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian\n",
    "\n",
    "In the **multivariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}^D$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^d\\left|\\Sigma\\right|}}\\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)^T\\Sigma^{-1}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$, or, $X\\sim N(\\mu,\\Sigma)$.\n",
    "\n",
    "* $\\mu$ is a $D$-dimensional mean vector\n",
    "* $\\Sigma$ is a $D\\times D$ covariance matrix\n",
    "* $\\left|\\Sigma\\right|$ denotes the determinant of $\\Sigma$\n",
    "* The precision parameter in a $d$-dimensional space is equal to $\\beta = \\Sigma^{-1}$\n",
    "\n",
    "Let $X=[X_1,X_2]$. The **covariance** $\\Sigma$ measures the amount of variance is each individual dimension, $X_1$ and $X_2$, as well as the amount of covariance between the two. We can write the covariance as\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{cov}(X_1,X_2) &= E\\bigl[\\left(X_1-E\\left[X_1\\right]\\right) \\left(X_2-E\\left[X_2\\right]\\right)\\bigr]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\text{var}(X_1) & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\text{var}(X_2)\\end{array}\\right]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\sigma^2_{X_1} & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\sigma^2_{X_2}\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* The **Pearson's correlation coefficient** between random variables $X_1$ and $X_2$ is defined as:\n",
    "\n",
    "$$ r = \\frac{\\operatorname{cov}(X_1,X_2)}{\\sqrt{\\text{var}(X_1)}\\sqrt{\\text{var}(X_2)}} = \\frac{\\text{cov}(X_1,X_2)}{\\sigma_{X_1} \\sigma_{X_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([0,0]) # mean vector\n",
    "cov  = np.array([[3,0],[0,1]]) # covariance matrix\n",
    "print(cov)\n",
    "G = stats.multivariate_normal(mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-6:6:100j, -6:6:100j]\n",
    "xy = np.column_stack([x.flat, y.flat])\n",
    "z = stats.multivariate_normal.pdf(xy, mean=mu, cov=cov)\n",
    "z = z.reshape(x.shape)\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x,y,z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                cmap=plt.cm.viridis)\n",
    "ax.set_xlabel('$x_1$',size=15)\n",
    "ax.set_ylabel('$x_2$',size=15)\n",
    "ax.set_zlabel('PDF $f_X(x_1,x_2)$',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Mathematica's demonstration [\"Joint Density of Bivariate Gaussian Random Variables\"](https://demonstrations.wolfram.com/JointDensityOfBivariateGaussianRandomVariables/) to better understand the role of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Univariate) Gaussian-Gaussian Conjugate Prior\n",
    "\n",
    "Consider the data likelihood $P(\\mathbf{x}|\\mu) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and the prior distribution $P(\\mu) \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$. The posterior probability will also be Gaussian-distributed:\n",
    "    \n",
    "\\begin{align*}\n",
    "P(\\mu|\\mathbf{x}) \\sim \\mathcal{N}\\left(\\frac{\\sum_{i=1}^N x_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2+\\sigma^2},\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Proof: \n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mu|\\mathbf{x}) &\\propto P(\\mathbf{x}|\\mu)P(\\mu) \\\\\n",
    "& = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(\\sum_{i=1}^N \\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right) -\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2} +\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\sum_{i=1}^N x_i^2-2\\sum_{i=1}^N x_i\\mu +\\mu^2N)}{\\sigma^2} +\\frac{\\mu^2-2\\mu\\mu_0 +\\mu_0^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{\\mu^2N}{\\sigma^2}+\\frac{\\mu^2}{\\sigma_0^2}\\right) - 2\\mu \\left( \\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) + \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right) \\left(\\mu^2 - 2\\mu\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1} \\right) \\right) \\exp\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right)\\\\\n",
    "&= \\dots\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Multivariate) Gaussian-Gaussian\n",
    "\n",
    "Let the multivariate Gaussian data likelihood with $D$-dimensions with mean $\\mu$ and covariance $\\beta\\mathbf{I}$ and a multivariate Gaussian \n",
    "\n",
    "\n",
    "prior distribution with mean $\\mu_0$ and covariance $\\Sigma_0$.\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{t}|\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}, \\beta\\mathbf{I}) \\\\\n",
    "P(\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}_0,\\Sigma_0)\n",
    "\\end{align*}\n",
    "\n",
    "The posterior distribution\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{w}|\\mathbf{t}) &\\sim \\mathcal{N}\\left(\\mathbf{\\mu}_N, \\Sigma_N\\right) \\\\\n",
    "\\mathbf{\\mu}_N &= \\Sigma_N \\left(\\Sigma_0^{-1}\\mathbf{\\mu}_0+\\beta\\mathbf{\\mathbf{X}}^T\\mathbf{t}\\right)\\\\\n",
    "\\Sigma_N^{-1} &= \\Sigma_0^{-1} + \\beta \\mathbf{\\mathbf{X}}^T\\mathbf{\\mathbf{X}}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{X}$ is the feature matrix of size $N \\times M$.\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\Sigma_0$?\n",
    "\n",
    "To simplify, let's assume the covariance of prior is **isotropic**, that is, it is a diagonal matrix with the same value along the diagonal, $\\Sigma_0 = \\alpha^{-1}\\mathbf{I}$. And, let also $\\mathbf{\\mu}_0 = [0,0]$, thus \n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_N = \\beta \\Sigma_N\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T\\mathbf{X}\\right)^{-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example of Online Updating of the Prior using Conjugate Priors (Multivariate Gaussian-Gaussian)\n",
    "\n",
    "Let's consider the example presented in the Bishop textbook (Figure 3.7 in page 155).\n",
    "\n",
    "Consider a single input variable $\\mathbf{x}$, a single target variable $\\mathbf{t}$ and a linear model of the form $y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1\\mathbf{x}$.\n",
    "Because this has just two parameters coefficients, $w=[w_0, w_1]^T$, we can plot the prior and posterior distributions directly in parameter space (2-dimensional parameter space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Let's generate some synthetic data from the function $f(x, a) = w_0 + w_1x$ with parameter values $w_0 = −0.3$ and $w_1 = 0.5$ by first choosing values of $x_n$ from the uniform distribution $U(x_n|−1, 1)$, then evaluating $f(x_n, \\mathbf{w})$, and finally adding Gaussian noise with standard deviation of $\\sigma = 0.2$ to obtain the target values $t_n$.\n",
    "\n",
    "$$t_n = f(x_n, \\mathbf{w}) + \\epsilon = -0.3 + 0.5 x_n + \\mathbf{\\epsilon}$$\n",
    "\n",
    "where $\\mathbf{\\epsilon}\\sim \\mathcal{N}(0,\\beta\\mathbf{I})$.\n",
    "\n",
    "* **Our goal is to recover the values of $w_0$ and $w_1$ from such data, and we will explore the dependence on the size of the data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some data, $\\{x_n,t_n\\}_{n=1}^N$, we can pose this problem in terms of **Regularized Least Squares**:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^1 w_i^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^2 \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\left(w_0^2 + w_1^2\\right)\\\\\n",
    "& \\Rightarrow \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align*}\n",
    "\n",
    "* Using **MAP**, we can rewrite our objective using the **Bayesian interpretation**:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{w}} \\max P(\\mathbf{\\epsilon}|\\mathbf{w})P(\\mathbf{w})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the data likelihood, $P(\\mathbf{\\epsilon}|\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma^2 = \\beta\\mathbf{I}$. And let's also consider the prior distribution, $P(\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2 = \\alpha^{-1}\\mathbf{I}$. Then, using the derivations from above, we can rewrite our optimization as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{\\mathbf{w}} \\max & \\mathcal{N}(\\mathbf{\\epsilon}|0,\\beta\\mathbf{I})\\mathcal{N}(\\mathbf{w}|\\mathbf{\\mu_0} ,\\alpha^{-1}\\mathbf{I}) \\\\\n",
    "\\propto\\arg_{\\mathbf{w}} \\max & \\mathcal{N}\\left( \\beta \\Sigma_N^{-1} \\mathbf{X}^T\\mathbf{t}, \\Sigma_N \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{\\mu}_0 = [0,0]$, $\\mathbf{X}$ is the polynomial feature matrix, and $\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T \\mathbf{X}\\right)^{-1}$ is the covariance matrix of the posterior distribution.\n",
    "\n",
    "Note that we **do not known** the parameters of the prior distribution ($\\mu_0$ and $\\sigma_0$ are unknown). The parameters of the prior distribution will have to be chosen by the user. And they will essentially *encode* any behavior or a priori knowledge we may have about the weights.\n",
    "\n",
    "* **Both our data likelihood and prior distributions are in a 2-dimensional space (this is because our *model order* is $M=2$ -- we have 2 parameters!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate data from $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is drawn from a zero-mean Gaussin distribution.\n",
    "\n",
    "* **The goal is to estimate the values $w_0=-0.3$ and $w_1=0.5$**\n",
    "* The feature matrix $\\mathbf{X}$ can be computed using the polynomial basis functions\n",
    "* **Parameters to choose:** $\\beta$ and $\\alpha$\n",
    "\n",
    "We want to implement this scenario for a case that we are getting more data every minute. As we get more and more data, we want to **update our prior distribution using our posterior distribution (informative prior)**, because they take the have the same distribution form. This is only possible because because Gaussian-Gaussian have a conjugate prior relationship. That is, the posterior distribution is also a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def likelihood_prior_func(beta = 2, alpha = 1, draw_num=(0,1,10,20,50,100)):\n",
    "    '''Online Update of the Posterior distribution for a Gaussian-Gaussian conjugate prior.\n",
    "    Parameter:\n",
    "    beta - variance of the data likelihood (of the additive noise)\n",
    "    alpha - precision value or 1/variance of the prior distribution\n",
    "    draw_num - number of points collected at each instance.\n",
    "    \n",
    "    This function will update the prior distribution as new data points are received.\n",
    "    The prior distribution will be the posterior distribution from the last iteration.'''\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true (unknown) weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space\n",
    "    rangeX = [-2, 2] # range of values for the input\n",
    "    step = 0.025 # distance between points\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step] # creates a grid of values for input samples\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    sigma = S0 # copying it so we can update it later\n",
    "    mean = [0,0] # mean for prior\n",
    "    \n",
    "    # Draws samples from Uniform(-1,1) distribution\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    # Generate the noisy target samples\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=np.sqrt(beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            \n",
    "            # INPUT DATA\n",
    "            #Feature Matrix (Polynomial features with M=2)\n",
    "            FeatureMatrix = np.array([draws[:draw_num[i]]**m for m in range(2)]).T\n",
    "            #Target Values\n",
    "            t = T[0:draw_num[i]]\n",
    "            \n",
    "            # POSTERIOR PROBABILITY\n",
    "            # Covariance matrix\n",
    "            sigma = np.linalg.inv(S0 + beta*FeatureMatrix.T@FeatureMatrix)\n",
    "            # Mean vector\n",
    "            mean = beta*sigma@FeatureMatrix.T@t\n",
    "            \n",
    "            # PARAMETER SPACE\n",
    "            # create a meshgrid of possible values for w's\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            \n",
    "            # Define the Gaussian distribution for data likelihood\n",
    "            p = multivariate_normal(mean=t[draw_num[i]-1], cov=beta)\n",
    "            # Initialize the PDF for data likelihood\n",
    "            out = np.empty(w0.shape)\n",
    "            # For each value (w0,w1), compute the PDF for all data samples\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "            \n",
    "            # Plot the data likelihood\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            # Add the current value for parameters w=(w0,w1)\n",
    "            ax.scatter(a,b, c='r',marker='x')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # PARAMETER SPACE\n",
    "        # create a meshgrid of possible values for w's\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        \n",
    "        # POSTERIOR PROBABILITY\n",
    "        # initialize the matrix with posterior PDF values\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        # for w0\n",
    "        pos[:, :, 0] = w0\n",
    "        # and for w1\n",
    "        pos[:, :, 1] = w1\n",
    "        # compute the PDF\n",
    "        p = multivariate_normal(mean=mean, cov=sigma)\n",
    "\n",
    "        #Show prior/posterior\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        # Add the value for parameters w=(w0,w1) that MAXIMIZE THE POSTERIOR\n",
    "        ax.scatter(a,b, c='r',marker='x')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # DATA SPACE\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "        for j in range(6):\n",
    "            # draw sample from the prior probability to generate possible values for parameters \n",
    "            w0, w1 = np.random.multivariate_normal(mean=mean, cov=sigma)\n",
    "            # Estimated labels\n",
    "            t = w0 + w1*X\n",
    "            # Show data space\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(FeatureMatrix[:,1], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# beta - variance of the data likelihood (for the additive noise)\n",
    "# alpha - precision value or 1/variance for the prior distribution\n",
    "# draw_num - number of points collected at each instance\n",
    "\n",
    "likelihood_prior_func(beta = 2, alpha = 1/2, draw_num=(0,1,2,20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
