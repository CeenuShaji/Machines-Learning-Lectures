{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1feb7185",
   "metadata": {},
   "source": [
    "# Lecture 23 Part 1 - PCA Application & Performance Measures for Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ab1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ea896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol',\n",
    "                   'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium',\n",
    "                   'Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols',\n",
    "                   'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219526b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Labels\n",
    "t = df_wine['Class label'].values\n",
    "\n",
    "# Feature Matrix\n",
    "X = df_wine.drop(['Class label'], axis=1).values\n",
    "print(X.shape)\n",
    "\n",
    "# Stratified partition of the data into training/test sets\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=t)\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849508d",
   "metadata": {},
   "source": [
    "Coming back to the wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85feacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_train = np.cov(X_train.T)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_train)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(13),df_wine.columns[1:],rotation=90)\n",
    "plt.yticks(range(13),df_wine.columns[1:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23272f",
   "metadata": {},
   "source": [
    "Building a function to implement PCA from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA(X, m, display=1):\n",
    "    '''This function implements PCA. The data matrix X is DxN matrix, \n",
    "    where D is the dimension and N the number of points'''\n",
    "    \n",
    "    D, N = X.shape\n",
    "    \n",
    "    # Demean the Data\n",
    "    data = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Covariance of the input  X\n",
    "    cov_mat = np.cov(data)\n",
    "    \n",
    "    # Find eigenvectors and eigenvalues \n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(cov_mat)\n",
    "    \n",
    "    # Sort eigenvectors by magnitude of eigenvalues\n",
    "    L = eigen_vals[::-1]\n",
    "    U = eigen_vecs[:,::-1]\n",
    "\n",
    "    # Linear transformation\n",
    "    A = U[:,:m].T\n",
    "    \n",
    "    #compute explained variance and visualize it\n",
    "    cumulative_var_exp=0\n",
    "    total = sum(L)\n",
    "    var_explained = [(i/total) for i in L]\n",
    "    cumulative_var_exp = np.cumsum(var_explained)\n",
    "    \n",
    "    if display:\n",
    "        plt.bar(range(1,D+1), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "        plt.step(range(1,D+1), cumulative_var_exp, alpha=0.5, where='mid', label='cumulative explained variance')\n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.legend(loc='best');\n",
    "        \n",
    "    return A, var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5872de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86daccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f59d2",
   "metadata": {},
   "source": [
    "The resulting plot indicates that the first principal component alone accounts for 40 percent of the variance. Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the data.\n",
    "\n",
    "Although the explained variance plot reminds us of the feature importance, we shall remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = ##\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_mat)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31c96c",
   "metadata": {},
   "source": [
    "## PCA with ```scikit-learn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b4f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de0e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af350b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix A = U.T is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(1,14),np.cumsum(pca.explained_variance_ratio_),c='r')\n",
    "plt.bar(range(1,14),pca.explained_variance_ratio_, alpha=0.5)\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce7ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = ##\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_mat)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(13),df_wine.columns[1:],rotation=90)\n",
    "plt.yticks(range(13),df_wine.columns[1:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(t_train), colors, markers):\n",
    "    plt.scatter(y_train_pca[t_train==l, 0], y_train_pca[t_train==l, 1],c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67633fd6",
   "metadata": {},
   "source": [
    "The training data is used to find the new features (eigenvectors). We can then represent the test set in this new feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b893de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa45f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b700c",
   "metadata": {},
   "source": [
    "## Example: Eigenfaces - Dataset Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces = fetch_olivetti_faces(return_X_y=False)\n",
    "\n",
    "# print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces.data # data matrix\n",
    "\n",
    "t = faces.target # target label\n",
    "\n",
    "X.shape, t.shape # 400 images, each of size 64x64=4096 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7222c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(40):\n",
    "    fig.add_subplot(7,6,i+1)\n",
    "    idx = np.random.choice(np.where(t==i)[0])\n",
    "    plt.imshow(X[idx,:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b987323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train.shape, t_train.shape, X_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ##\n",
    "pca.fit_transform(X_train_scaled);\n",
    "\n",
    "plt.plot(100*np.cumsum(pca.explained_variance_ratio_), '-o')\n",
    "plt.xlabel('Principal Components',size=15)\n",
    "plt.ylabel('Cumulative Explained Variance, in %', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ab779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d16d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "359a07cf",
   "metadata": {},
   "source": [
    "In order to explain 90% of the variance in the data, we need to preserve 63 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe91ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec374980",
   "metadata": {},
   "source": [
    "Let's project to 2-D so we can plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (unsupervised)\n",
    "\n",
    "pca = ##\n",
    "ypca = ##\n",
    "\n",
    "ypca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794779da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (supervised)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = ##\n",
    "ylda = ##\n",
    "\n",
    "ylda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(ypca[:,0], ypca[:,1], c=t_train, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('PC 1', size=15)\n",
    "plt.ylabel('PC 2', size=15)\n",
    "plt.title('PCA')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(ylda[:,0], ylda[:,1], c=t_train, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('LD 1', size=15)\n",
    "plt.ylabel('LD 2', size=15)\n",
    "plt.title('LDA')\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263883d",
   "metadata": {},
   "source": [
    "Not that the 40 classes are overlapping in the linear projection space. This is because PCA is **unsupervised**, it does use the class labels *anywhere* in finding the matrix for linear projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa169171",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe47ced",
   "metadata": {},
   "source": [
    "To apply this transformation in the test set, simply multiply the resultant modal matrix with the scaled test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set using the linear transformation found with the training data\n",
    "ypca_test = ##\n",
    "\n",
    "ypca_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ac700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(ypca_test[:,0], ypca_test[:,1], c=t_test, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('Principal Component 1', size=15)\n",
    "plt.ylabel('Principal Component 2', size=15)\n",
    "plt.title('Test Set')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893013d",
   "metadata": {},
   "source": [
    "You can access the linear transformation $\\mathbf{A} = \\mathbf{U}^T$ using the method ```components_```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ##\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157caf79",
   "metadata": {},
   "source": [
    "Note that the eigenvectors are described in the original space, that is, they are 4096-dimensional!\n",
    "\n",
    "Since we are working with images, we can reshape them back to a $64 \\times 64$ image and see what are the regions in the image with maximum explained variance! This is called the **eigenfaces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = ##\n",
    "\n",
    "yyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(yyy[:,0], yyy[:,1], c=t_test, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('Principal Component 1', size=15)\n",
    "plt.ylabel('Principal Component 2', size=15)\n",
    "plt.title('Test Set')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2886962",
   "metadata": {},
   "source": [
    "Let's now recover 16 eigenvectors and plot them as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926acf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(abs(pca.components_[0,:].reshape(64,64)))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 16\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "ypca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(n_components):\n",
    "    fig.add_subplot(4,4,i+1)\n",
    "    plt.imshow(abs(pca.components_[i,:].reshape(64,64)),cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8dc61",
   "metadata": {},
   "source": [
    "The eigenvectors are describing the regions in the 64x64 image that explain the most variance. the more eigenvectors are kept, the better a reconstruction image will be produced.\n",
    "\n",
    "For example, let's reconstruct the images in the dataset using the top 16 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = ##\n",
    "\n",
    "L, U = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8c892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eigenvectors = ##\n",
    "\n",
    "P = U[:, :N_eigenvectors]\n",
    "\n",
    "X_proj = X_train_scaled@P\n",
    "X_reconstruct = X_proj@np.linalg.pinv(P)\n",
    "\n",
    "X_reconstruct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434258af",
   "metadata": {},
   "source": [
    "Since the projection is given by:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{A}\\mathbf{X}$$\n",
    "\n",
    "In order to recover $\\mathbf{X}$, we need to left-multiply by the pseudo-inverse of $\\mathbf{A}$:\n",
    "\n",
    "$$\\hat{\\mathbf{X}} = \\mathbf{A}^\\dagger\\mathbf{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a44da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "\n",
    "ypca = pca.transform(X_train_scaled)\n",
    "X_reconstruct_skl = pca.inverse_transform(ypca)\n",
    "\n",
    "X_reconstruct_skl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5401690",
   "metadata": {},
   "source": [
    "We also need to bringing back to the original scaling: multiplying by the standard deviation and adding the sample mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba76b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed = scaler.inverse_transform(X_reconstruct)\n",
    "\n",
    "X_reconstructed_skl = scaler.inverse_transform(X_reconstruct_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "idx = np.random.choice(range(X_reconstructed.shape[0]),replace=False,size=N)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "j=1\n",
    "for i in range(N):\n",
    "    fig.add_subplot(2,N,j)\n",
    "    plt.imshow(X_train[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image');\n",
    "\n",
    "    fig.add_subplot(2,N,j+N)\n",
    "    plt.imshow(X_reconstructed[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Reconstructed Image');\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18964cf6",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eigenvectors = ##\n",
    "\n",
    "pca = PCA(n_components=N_eigenvectors)\n",
    "ypca = pca.fit_transform(X_train_scaled)\n",
    "X_reconstruct = pca.inverse_transform(ypca)\n",
    "X_reconstructed = scaler.inverse_transform(X_reconstruct)\n",
    "\n",
    "N = 5\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "idx = np.random.choice(range(X_reconstructed.shape[0]),replace=False,size=N)\n",
    "j=1\n",
    "for i in range(N):\n",
    "    fig.add_subplot(2,N,j)\n",
    "    plt.imshow(X_train[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image');\n",
    "\n",
    "    fig.add_subplot(2,N,j+N)\n",
    "    plt.imshow(X_reconstructed[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Reconstructed Image');\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011844e",
   "metadata": {},
   "source": [
    "Still some compression loss but much better representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe6618",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33540d91",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcffbe1",
   "metadata": {},
   "source": [
    "A key step in machine learning algorithm development and testing is determining a good error and evaluation metric.\n",
    "\n",
    "**Evaluation metrics** help us to estimate how well our model is trained and it is important to pick a metric that matches our overall goal for the system.\n",
    "\n",
    "Some common evaluation metrics include precision, recall, receiver operating curves, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25016393",
   "metadata": {},
   "source": [
    "## Classification Accuracy and Error\n",
    "\n",
    "Classification accuracy and e the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "* **Classification accuracy** is defined as the number of correctly classified samples divided by all samples:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{accuracy} = \\frac{N_{\\text{corr}}}{N}\n",
    "\\end{align*}\n",
    "\n",
    "where $N_{\\text{corr}}$ is the number of correct classified samples and $N$ is the total number of samples.\n",
    "\n",
    "* **Classification error** is defined as the number of incorrectly classified samples divided by all samples:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{error} = \\frac{N_{\\text{miss}}}{N}\n",
    "\\end{align*}\n",
    "\n",
    "where $N_{\\text{miss}}$ is the number of misclassified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Classification accuracy is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.\n",
    "\n",
    "## Example 1: Fish Dataset\n",
    "Suppose there is a 3-class classification problem, in which we would like to classify each training sample (a fish) to one of the three classes (A = salmon or B = sea bass or C = cod).\n",
    "\n",
    "Let's assume there are 150 samples, including 30 salmon, 40 sea bass and 80 cod. Suppose our model misclassifies 4 salmon, 2 sea bass and 5 cod.\n",
    "\n",
    "* The classification accuracy (ACC) of our binary classification model is calculated as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{ACC} = \\frac{26 + 38 + 75}{30 + 40 + 80} = \\frac{139}{150} \\approx 92.7 \\%\n",
    "\\end{align*}\n",
    "\n",
    "* The prediction error is calculated as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{error} = \\frac{4 + 2 + 5}{30+40+80} = \\frac{11}{150} \\approx 7.3 \\%\n",
    "\\end{align*}\n",
    "\n",
    "* The classification accuracy doesn't really gives an insight on which class is being misclassified the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5795a",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix summarizes the classification accuracy across several classes. It shows the ways in which the classification model is confused when it makes predictions, allowing visualization of the performance of our algorithm. \n",
    "\n",
    "Generally, each row represents the instances of a actual class while each column represents the instances in an predicted class.\n",
    "\n",
    "If the classifier is trained to distinguish between salmon, sea bass and cod. We can summarize the prediction result in the confusion matrix as follows:\n",
    "\n",
    "|actual/predict|    salmon    |    sea bass  |      cod     |\n",
    "|--------------|--------------|--------------|--------------|\n",
    "|    salmon    |      26      |       2      |       2      |\n",
    "|    sea bass  |       2      |       38     |       0      |\n",
    "|      cod     |       2      |       3      |       75     |\n",
    "\n",
    "\n",
    "In this confusion matrix, of the 30 salmons (row 1), the classifier predicted that 26 are labeled salmon correctly, 2 are wrongly labeled as sea bass, and another 2 are wrongly labeled as cod. \n",
    "\n",
    "All correct predictions are located in the diagonal of the table. So it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfd15",
   "metadata": {},
   "source": [
    "## Precision, Recall & Fall-Out\n",
    "\n",
    "We are often looking to discriminate between observations with a specific binary outcome, for example, event or no event. In our example, the fish company would like to produce salmon can but the harvest contains all three species. In this way,\n",
    "we can assign the event (salmon) as \"positive\" and no-event (not salmon) as \"negative\".\n",
    "\n",
    "The confusion matrix for this two-class classification problem is:\n",
    "\n",
    "|actual/predict|    salmon    |  non-salmon  |\n",
    "|--------------|--------------|--------------|\n",
    "|    salmon    |      26      |       4      |\n",
    "|  non-salmon  |       4      |      116     |\n",
    "\n",
    "* **True positive (TP):** correctly predicting positive events\n",
    "* **False positive (FP):** incorrectly calling positive to a negative event\n",
    "* **True negative (TN):** correctly predicting negative events\n",
    "* **False negative (FN):** incorrectly labeling negative to a positive event\n",
    "\n",
    "*In this salmon/non-salmon classification problem, what are the TP, FP, TN, FN values?*\n",
    "\n",
    "|actual/predict|   Positive   |   Negative   |\n",
    "|--------------|--------------|--------------|\n",
    "|   Positive   |      TP      |      FN      |\n",
    "|   Negative   |      FP      |      TN      |\n",
    "\n",
    "* **Precision**, also called Positive Predictive Value (PPV), is the performance of detection\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\text{PPV} = \\frac{TP}{TP + FP}\n",
    "\\end{align*}\n",
    "\n",
    "* **Recall**, also called True Positive Rate (TPR) or Sensitivity, is the probability of detection\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} = \\text{TPR} = \\text{Sensitivity} = \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "* **Fall-out**, also called False Positive Rate (FPR), is the probability of false alarm\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Fall-out} = \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "\\end{align*}\n",
    "\n",
    "* **Specificity**, also called True Negative Rate (TNR), is the probability of negative events detection\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "\\end{align*}\n",
    "\n",
    "* **F1-score**, also called F-score or F-measure, is a measure of a model's accuracy. It considers both the precision and the recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-score} = 2\\times\\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}\n",
    "\n",
    "* Learn about many other measures on this [Wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and [Scikit-Learn's Classification Metrics Module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349197b8",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "**Receiver Operating Characteristic (ROC) curve** is the plot between the true positive rate (TPR) and the false positive rate (FPR), where the TPR is defined as the y-axis and FPR is defined as the x-axis.\n",
    "\n",
    "* ROC curves were first developed for RADAR systems, hence the name.\n",
    "\n",
    "* Given a binary classifier and its threshold, the (x,y) coordinates of ROC space can be calculated from all the prediction result. You trace out a ROC curve by varying the threshold to get all of the points on the ROC.\n",
    "\n",
    "* The diagonal between (0,0) and (1,1) separates the ROC space into two areas, which are left up area and right bottom area. The points above the diagonal represent good classification (better than random guess) which below the diagonal represent bad classification (worse than random guess).\n",
    "\n",
    "* *What is the perfect prediction point in a ROC curve?*\n",
    "\n",
    "\n",
    "### Area Under the Curve (AUC)\n",
    "\n",
    "**Area Under Curve (AUC)** is a common measure of how good a test is. It is simply the area under the ROC curve. Random guessing can achieve the diagonal line, so the minimum AUC is 1/2. The maximum AUC is 1, which is achieved by a test that is always right; the ROC curve is along the left and top axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce8a0a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc49383",
   "metadata": {},
   "source": [
    "## <font color='blue'>Example</font>\n",
    "\n",
    "1. Suppose you have a target detection task that you would like to evaluate using ROC curve analysis. You emplaced 10 targets and collected aerial hyperspectral imagery over 10 $km^2$. Then, suppose you ran a set of alarm generation and target detection algorithms over the collected data. Your algorithms produced the following list of alarm confidence values. You have already matched each of these alarms to a location on the ground and compared them with you ground truth. True targets, based on your ground truth, are marked with a \"T\" in the second column. Draw the associated ROC cure for these results.\n",
    "\n",
    "Alarm confidence values |  0.91  |  0.90  |  0.80  |  0.79  |  0.77  |  0.75  |  0.50  |  0.40  |  0.39  |  0.38  |  0.37  |  0.25  |  0.10  |  0.09  |  0.01  |\n",
    "------------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
    "    Ground truth        |   T    |   T    |        |   T    |        |        |        |   T    |        |        |        |        |        |   T    |        |\n",
    "\n",
    "\n",
    "2. Suppose you were segmenting a data set into three classes (e.g., vegetation, man-made materials, sand) and wanted to evaluate your results. Would using a ROC curve be an appropriate method for evaluation? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8b43b",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
