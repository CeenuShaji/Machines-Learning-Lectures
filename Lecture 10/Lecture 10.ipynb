{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder - Midterm Exam\n",
    "\n",
    "## Final Exam: <font color=blue>Monday, October 10</font> \n",
    "\n",
    "### Final Exam Review Lecture: <font color=orange>Tuesday, October 4 (in-class)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Section|\tDate |\tTime |\tLocation |\tModality |\n",
    "| -- | -- | --- | ---| ---|\n",
    "| EEL5840 On-campus |\tMonday, October 10 |\t7:20 PM - 9:20 PM |\tWEIM 1064 |\tin-person |\n",
    "| EEL5840 EDGE/Online |\tMonday, October 10 |\twithin 24 hours |\tHonorlock |\tonline |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Details\n",
    "\n",
    "* **Modality**: The exam will be via Honorlock for EDGE and online sections and will be in-person for campus sections\n",
    "\n",
    "* **Coverage**: lectures 1-12 (modules 1-5)\n",
    "\n",
    "* **Practice exam**: available in the [Assignment-Solutions repo](https://github.com/UF-EEL5840-F22/Assignment-Solutions)\n",
    "    * Solutions will be posted later this week\n",
    "    \n",
    "* **Allowed Material**\n",
    "    * 1-page letter-sized of formulas (front and back, handwritten or typed)\n",
    "    * Scientific calculator\n",
    "\n",
    "* **Total time**: 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class Midterm Review\n",
    "\n",
    "Post questions to be answered during the in-class midterm review:\n",
    "\n",
    "### [Midterm Exam Review and Questions](https://ufl.instructure.com/courses/464118/discussion_topics/3625863)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Topic: <font color=blue>Handwritten Mathematical Symbols Classification</font> \n",
    "\n",
    "### Due: <font color=orange>Tuesday, December 6 @ 11:59 PM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-class discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 - Mixture Models & The Expectation-Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the data for a *single class* looks like the plot below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples = 1500, centers = 5)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1]); plt.axis([-15,15,-15,15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a single Gaussian distribution, we would obtain a very poor estimate of the true underlying data likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models\n",
    "\n",
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "A **Gaussian Mixture Model** or **GMM** is a probabilistic model that assumes a data likelihood to be a weighted sum of Gaussian distributions with unknown parameters.\n",
    "\n",
    "$$p(\\mathbf{x}|\\Theta) = \\sum_{k=1}^K \\pi_k N(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)$$\n",
    "\n",
    "where $\\Theta=\\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$, $0 \\leq \\pi_k \\leq 1$ and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "* When standard distributions (such as Gamma, Exponential, Gaussian, etc.) are not sufficient to characterize a *complicated* data likelihood, we can instead characterize it as the sum of weighted Gaussians distributions\n",
    "\n",
    "* Another way that GMMs are most commonly used for is to partition data in subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a Data Likelihood as a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMMs can be used to learn a complex distribution that represent a dataset. Thus, it can be used within the probabilistic generative classifier framework to model complex data likelihoods.\n",
    "\n",
    "* GMMs are also commonly used for **clustering**. Here a GMM is fit to a dataset with the goal of partitioning it into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 1</font>**\n",
    "\n",
    "Describe the **observed data likelihood**, $\\mathcal{L}^o$. As seen last class:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 2</font>**\n",
    "\n",
    "Describe the log-likelihood function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\ln\\left(\\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\\right)\\\\\n",
    "\\iff \\mathcal{L} &= \\sum_{i=1}^N \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 3</font>**\n",
    "\n",
    "Optimize for the parameters $\\Theta=\\{\\pi_k, \\mu_k,\\Sigma_k\\}_{k=1}^K$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} = 0, \\text{ and }, \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = 0\n",
    "\\end{align*}\n",
    "\n",
    "but this is a difficult problem to maximize!\n",
    "\n",
    "* A common approach for estimating the parameters of a GMM given a data set is by using the **Expectation-Maximization (EM) algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Mixture Models\n",
    "\n",
    "* **Data Representation and Inference**. Represent *any* data set using a complex distribution. You may want to describe your data with a distribution but the data does not fit into a standard form (e.g. Gamma, Gaussian, Exponential, etc.), then you can use a (Gaussian) Mixture Model to describe the data. Having a representation of your data in a distribution form is a very powerful tool that, other than having a model of the data, allows you infer about new samples, make predictions, etc.\n",
    "    * Having a parametric form for a real world model, allows us to apply it in simulation and use it for designing/optimizing decision-making solutions.\n",
    "\n",
    "* **Clustering.** Partition the data into groups. Note that in the GMMs formulation we did not add the concept of labels/targets. So GMMs are an **unsupervised learning** model. It represents the data with a very complex likelihood and then we can decompose that likelihood to partition the data into categories. This is also known as **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Example: Censored Data</font>**\n",
    "\n",
    "Consider the observation of i.i.d. samples $x_1, x_2, \\dots, x_N$ from the data likelihood $p(\\mathbf{x}|\\Theta)$ and we want to estimate the parameters (using MLE approach, for example)\n",
    "\n",
    "\\begin{align*}\n",
    "& \\arg_{\\Theta} \\max p(\\mathbf{x}|\\Theta) \\\\\n",
    "= & \\arg_{\\Theta} \\max \\prod_{i=1}^N p(x_i|\\Theta) \n",
    "\\end{align*}\n",
    "\n",
    "Now suppose the data samples $x_1, x_2, \\dots, x_N$ are **censored**.\n",
    "\n",
    "For example, suppose we observe i.i.d. samples, $x_1, x_2, \\dots, x_N$, from some sensor $f(\\mathbf{x})$. This sensor returns censored data in the form,\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}) = \\begin{cases} x, &&\\text{ if }  x < a \\\\ a && \\text{ if }x\\geq a \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "This means that we see $x_1, x_2,\\dots, x_m$ (less than $a$) and we do not see $x_{m+1}, x_{m+2}, \\dots, x_N$ which are censored and set to $a$.\n",
    "\n",
    "* An example of such censored data would be a scale that can only measure weight up to 120 lbs. Any measurements above 120 would be *censored* at 120.\n",
    "\n",
    "* Given this censored data, we want to estimate the mean of the data as if the data was uncensored.\n",
    "\n",
    "For this case, we can write our *observed* data likelihood as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^m p(x_i|\\Theta) \\prod_{j=m+1}^N \\int_a^{\\infty} p(x_j|\\Theta) dx_j\n",
    "\\end{align*}\n",
    "\n",
    "The data likelihood would be very difficult to maximize to solve for $\\Theta$.\n",
    "\n",
    "If only we *knew* what the missing/censored data, the problem would be easy to solve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Latent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Expectation-Maximization** or **EM** algorithm is used to find the Maximum Likelihood Estimators (MLE) (or MAP estimators) for model parameters when data is incomplete, has missing data points, or has unobserved (hidden) latent variables (such as the case of censored data). \n",
    "\n",
    "* For all of these cases, the MLE optimization is very difficult to obtain by simply taking the derivative and solve for the parameters.\n",
    "\n",
    "**<font color=orange>Step 1</font>**\n",
    "The first step of EM is to characterized the observed likelihood $\\mathcal{L}^0$.\n",
    "\n",
    "**<font color=orange>Step 2</font>**\n",
    "Introduce *hidden latent variables* (also referred to as *hidden variables*) that simplify the observed data likelihood, $\\mathcal{L}$.\n",
    "\n",
    "**<font color=orange>Step 3</font>**\n",
    "Use the hidden variables to define the *complete likelihood* $\\mathcal{L}^c$.\n",
    "\n",
    "With this, we build the EM optimization function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{z,\\Theta} \\max Q(\\Theta,\\Theta^t)\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\Theta,\\Theta^t) = E[\\ln(\\mathcal{L^c})|X,\\Theta^t]\n",
    "\\end{align*}\n",
    "\n",
    "$E[\\bullet]$ denotes expected value and $t$ denotes *iteration*. At $t=0$, we start with random values for the parameters $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this, the EM algorithm will iterate between the E-step and the M-step:\n",
    "\n",
    "1. **<font color=blue>E-step</font> (Expectation step)** Estimate the hidden variables. While holding $\\Theta$ fixed, find the variables $z$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "2. **<font color=blue>M-step</font> (Maximization step)** Estimate the parameters of the complete data likelihood $\\mathcal{L}^c$. While holding the newly found variables $z$, find the best values for the parameters $\\Theta$ that maximize $E[\\ln(\\mathcal{L^c})]$.\n",
    "\n",
    "and it keeps iterating between E-step and M-step until convergence or until a certain number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Optimization\n",
    "\n",
    "* EM is an **alternating optimization** algorithm, as it alternates between E-step and M-step in order to find the hidden variables and best set of parameters, respectively.\n",
    "    * In the first step ($t=0$), EM will start with a random guess for the value of the parameters $\\Theta$ in order to perform the E-step.\n",
    "    * What is the problem with alternating optimization algorithms in general?\n",
    "\n",
    "* EM is a general algorithm that can be applied to a variety of problems (not just the examples we are learning today). \n",
    "\n",
    "* EM is heavily tied with Maximum Likelihood Estimation (MLE). It is commonly used to simplify difficult MLE problems.\n",
    "    * It was originally introduced by Dempster, Laird, and Rubin in 1977 in a paper called [\"Maximum Likelihood from Incomplete Data via the EM Algorithm\"](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing GMM with the EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data likelihood for a Gaussian Mixture Model (GMM) is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What hidden variables can we add to simplify this problem?**\n",
    "\n",
    "* In this example, a hidden variable can be the label of the Gaussian from which $x_i$ was drawn from.\n",
    "\n",
    "\\begin{align*}\n",
    "z_i: \\text{label of the Gaussian from which $x_i$ was drawn from}\n",
    "\\end{align*}\n",
    "\n",
    "If we know $z_i$, then for each data point we know which Gaussian it was drawn from along with its respective parameter $\\mu_{z_i}$ and $\\Sigma_{z_i}$ and its respective weight $\\pi_{z_i}$. So, each data point would had been drawn from $\\pi_{z_i}N(x_i|\\mu_{z_i}, \\Sigma_{z_i})$. \n",
    "\n",
    "Then, assuming we have $\\{z_i\\}_{i=1}^N$, we can write the complete data likelihood:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^c = \\prod_{i=1}^N \\pi_{z_i}\\mathcal{N}(x_i|\\mathbf{\\mu}_{z_i},\\Sigma_{z_i})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate between the **E-step** and **M-step** of the EM algorithm until we find convergence or we have reached a threshold for a number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Function\n",
    "\n",
    "We can now extent the optimization function:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\Theta,\\Theta^t) &= E[\\ln(\\mathcal{L^c})|X,\\Theta^t] \\\\\n",
    "&= \\sum_{\\mathbf{z}} \\ln(\\mathcal{L^c}) P(z|X,\\Theta^t) \\\\\n",
    "&= \\sum_{z_i=1}^K \\ln(\\mathcal{L^c}) P(\\mathbf{z}_i|\\mathbf{x}_i,\\Theta^t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
