{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1feb7185",
   "metadata": {},
   "source": [
    "# Lecture 22 - Dimensionality Reduction with Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ab1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ea896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol',\n",
    "                   'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium',\n",
    "                   'Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols',\n",
    "                   'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219526b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Labels\n",
    "t = df_wine['Class label'].values\n",
    "\n",
    "# Feature Matrix\n",
    "X = df_wine.drop(['Class label'], axis=1).values\n",
    "print(X.shape)\n",
    "\n",
    "# Stratified partition of the data into training/test sets\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=t)\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c5b36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d5c80",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "A very common approach (and one of the simplest approaches) to **dimensionality reduction** is **Principal Component Analysis** (or **PCA**). \n",
    "\n",
    "* PCA takes data from *sensor coordinates* to *data centric coordinates* using linear transformations.\n",
    "\n",
    "PCA uses a **linear transformation** to **minimize the redundancy** of the resulting transformed data (by ending up with data that is uncorrelated).\n",
    "* This means that every transformed dimension is more informative.\n",
    "* In this approach, the dimensionality of the space is still the same as the original data, but the space of features are now arranged such that they contain the most information.\n",
    "\n",
    "If we wish to reduce dimensionality of our feature space, we can choose only the features that carry over the most information in the linearly transformed space.\n",
    "* In other words, PCA will find the underlying **linear manifold** that the data is embedded in.\n",
    "\n",
    "**PCA finds the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one**. \n",
    "\n",
    "There are a couple of points-of-view on how to find $A$: \n",
    "\n",
    "1. Maximum Variance Formulation\n",
    "2. Minimum-error Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7cd7c",
   "metadata": {},
   "source": [
    "## 1. PCA as Maximum Variance Formulation\n",
    "\n",
    "Consider the data $X$ comprised on $N$ data samples in a D-dimensional space, so $X$ is a matrix of size $D\\times N$.\n",
    "\n",
    "The **first step** in PCA is to centralize or demean $X$. This will is to guarantee that all features will have the same impact and not weight more only because their range of values is much larger (example, age vs income).\n",
    "\n",
    "* Without loss of generality, let's assume that we subtracted the mean to the input data, $X$. Now, $X$ has zero mean.\n",
    "\n",
    "The **second step** is to find the linear transformation $A$ that transforms $X$ to a space where features are:\n",
    "1. Uncorrelated (preserve all dimensions)\n",
    "2. reduced (dimensionality reduction)\n",
    "\n",
    "\\begin{align*}Y = AX\\end{align*}\n",
    "\n",
    "where $A$ is a $D\\times D$ matrix, $X$ is a $D\\times N$ data matrix and therefore $Y$ is also a $D\\times N$ transformed data matrix.\n",
    "\n",
    "The variance of the transformed data $Y$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "R_y &= E[YY^T] \\\\\n",
    "&= E[AX(AX)^T] \\\\\n",
    "&= E[AXX^TA^T] \\\\\n",
    "&= AE[XX^T]A^T \\\\\n",
    "&= AR_xA^T\n",
    "\\end{align*}\n",
    "\n",
    "Note that we are computing the variance along the dimensions of $Y$, therefore, $R_y$ is a $D\\times D$ matrix. Similarly, $R_X$ represents the covariance of the data $X$. Covariances matrices are symmetric therefore $R_X=R_X^T$ and $R_Y=R_Y^T$.\n",
    "\n",
    "Similarly, $R_X$ represents the covariance of the data $X$. \n",
    "\n",
    "If we write $A$ in terms of vector elements:\n",
    "\n",
    "\\begin{align*}A = \\left[\\begin{array}{cc}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}^T\\\\\n",
    "\\overrightarrow{a_{2}}^T\n",
    "\\end{array}\\right]\\end{align*}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align*}\n",
    "R_y &= \\left[\\begin{array}{c}a_{1}^T\\\\a_{2}^T\\end{array}\\right] R_X \\left[\\begin{array}{cc}a_{1} & a_{2}\\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{cc} a_1^T R_X a_1 & a_1^T R_X a_2\\\\ a_2^T R_X a_1 & a_2^T R_X a_2\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* If we want to represent the data in a space in which the features are **uncorrelated**, what shape does the covariance matrix have to take?\n",
    "\n",
    "Diagonal! Why?\n",
    "\n",
    "* Can we use the eigenvectors of $R_X$ as our linear transformation $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcef50e",
   "metadata": {},
   "source": [
    "Consider the case where we are trying to project the data $X$ into a 1-dimensional space, so we are trying to find the direction $a_1$ where maximal data variance is preserved:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{a_1} \\max a_1^T R_X a_1\\end{align*}\n",
    "\n",
    "We want this solution to be bounded (considering $a_1 = \\infty$ would maximize), so we need to constraint the vector to have norm 1\n",
    "\n",
    "\\begin{align*}\\Vert a_1\\Vert_2^2 = 1 \\iff a_1^T a_1 = 1  \\iff a_1^T a_1 - 1 = 0\\end{align*}\n",
    "\n",
    "Then, we using Lagrange Optimization:\n",
    "\n",
    "\\begin{align*}\\mathcal{L}(a_1, \\lambda_1) = a_1^T R_X a_1 - \\lambda_1 (a_1^T a_1 - 1)\\end{align*}\n",
    "\n",
    "Solving for $a_1$:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial a_1} = 0 \\iff 2R_X a_1 - 2\\lambda_1 a_1 = 0 \\iff R_X a_1 = \\lambda_1 a_1\n",
    "\\end{align*}\n",
    "\n",
    "* Does this look familiar?\n",
    "\n",
    "This is stating that $a_1$ must be an eigenvector of $R_X$!\n",
    "\n",
    "So coming back to the question \"Can we use the eigenvectors of $X$ as our linear transformation $A$?\" YES!\n",
    "\n",
    "* If we left multiply by $a_1^T$ and make use of $a_1^Ta_1 = 1$:\n",
    "\n",
    "\\begin{align*}a_1^T R_X a_1 = \\lambda_1\\end{align*}\n",
    "\n",
    "**So the variance will be maximum when we set the project direction $a_1$ equal to the eigenvector having the largest eigenvalue $\\lambda_1$**.\n",
    "\n",
    "* This eigenvector is known as the firt **principal component**.\n",
    "\n",
    "* As you may anticipate, the linear trasnformation $A$ will the be a matrix whose row entries are **sorted** the eigenvectors (sorted by their correspondent eigenvalue).\n",
    "\n",
    "\\begin{align*}A &= \\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}^T\\\\\n",
    "\\overrightarrow{a_{2}}^T\n",
    "\\end{array}\\right]\\end{align*}\n",
    "\n",
    "where $a_1$ and $a_2$ are eigenvectors of $R_X$ with correspondent eigenvalues $\\lambda_1$ and $\\lambda_2$ with $\\lambda_1>\\lambda_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404737f7",
   "metadata": {},
   "source": [
    "## 2. PCA as Minimum-error Formulation\n",
    "\n",
    "We can also look at PCA as a minimization of mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191ca377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "width": 300
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/Figure12.2.png',width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f12083",
   "metadata": {},
   "source": [
    "Consider $X$ and a D-dimensional orthogonal basis $\\mathbf{u}$:\n",
    "\n",
    "\\begin{align*}\\hat{x} = \\sum_{i=1}^m y_i a_i\\end{align*}\n",
    "\n",
    "where $m<D$.\n",
    "\n",
    "\\begin{align*}y_j = x^Ta_j \\end{align*}\n",
    "\n",
    "where $A^TA = I$.\n",
    "\n",
    "We want to minimize the residual error:\n",
    "\n",
    "\\begin{align*}\\epsilon = x - \\hat{x} = \\sum_{i=m+1}^D y_i a_i\\end{align*}\n",
    "\n",
    "* The objective function we will use is the mean square residual:\n",
    "\n",
    "\\begin{align*}\n",
    "J &= E\\left[ \\|\\epsilon\\|^2_2\\right]\\\\\n",
    "&= E\\left[\\left( \\sum_{i=m+1}^D y_ia_i\\right)\\left( \\sum_{i=m+1}^D y_i a_i\\right) \\right]\\\\\n",
    "&=\\sum_{j=m+1}^D E [y_j^2], \\text{because }a_i^Ta_j=0, \\forall i\\neq j \\text{ and }a_i^Ta_j=1, \\forall i=j\\\\\n",
    "&=\\sum_{j=m+1}^D E [(a_j^T\\mathbf{x})(\\mathbf{x}^Ta_j)]\\\\\n",
    "&= \\sum_{j=m+1}^D a_j^T E[\\mathbf{x}\\mathbf{x}^T]a_j\\\\\n",
    "&= \\sum_{j=m+1}^D a_j^T R_x a_j\n",
    "\\end{align*}\n",
    "\n",
    "Minimize the error and incorporate Lagrange parameters for $A^TA=I$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial a_j} &= 2(R_x a_j - \\lambda_j a_j) = 0\\\\\n",
    "R_x a_j &= \\lambda_j a_j\n",
    "\\end{align*}\n",
    "\n",
    "So, the sum of the error is the sum of the eigenvalues of the unused eigenvectors.  So, we want to select the eigenvectors with the $m$ largest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464cf437",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b3eac",
   "metadata": {},
   "source": [
    "# Steps of PCA\n",
    "\n",
    "Consider the data $X$ with $N$ data points defined in a $D$-dimensional space, that is, $X$ is a $D\\times N$ matrix.\n",
    "\n",
    "1. Subtract the mean, $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$.\n",
    "\n",
    "2. Compute the covariance matrix $R_X$ (by definition, the covariance already subtracts the data's mean). This matrix is of size $D\\times D$.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues of the matrix $R_X$, and store the sorted eigenvectors ($e_i$) in decreasing eigenvalue ($\\lambda_i$) order.\n",
    "\n",
    "4. Build the modal matrix $\\mathbf{U} = \\left[\\begin{array}{c} \\mathbf{e_{1}} &| & \\mathbf{e_{2}} &|\\dots |& \\mathbf{e_{D}}\\end{array}\\right]$, where all the (unit-length) eigenvectors are stacked in columns, sorted by their respective eigenvalues, i.e., $\\lambda_1>\\lambda_2>\\dots>\\lambda_D$.\n",
    "\n",
    "    * For **uncorrelating the data**, preserve all $D$ eigenvectors. Hence $\\mathbf{U}$ is a $D \\times D$ matrix.\n",
    "    * For **dimensionality reduction**, keep the top $M$ eigenvectors with the largest eigenvalues. Hence $\\mathbf{U}$ is a $D \\times M$ matrix.\n",
    "\n",
    "5. Apply the linear transformation: $\\mathbf{y} = \\mathbf{U}^T \\mathbf{X}$. Here $\\mathbf{y}$ is a matrix of size $M \\times N$, where $M\\leq D$.\n",
    "\n",
    "Note that the formal definition of covariance already accounts for demeaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28eca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plotvec(*argv):\n",
    "    colors=['k','b','r','g','c','m']\n",
    "    xmin=0\n",
    "    xmax=-1000000\n",
    "    ymin=0\n",
    "    ymax=-1000000\n",
    "    origin=[0,0]\n",
    "#     plt.figure()\n",
    "    for e in enumerate(argv):\n",
    "        i=e[0]\n",
    "        arg=e[1]\n",
    "        plt.quiver(*origin,*arg,angles='xy',scale_units='xy',scale=1,\n",
    "                   color=colors[i%len(colors)])\n",
    "        xmin=min(xmin,arg[0])\n",
    "        xmax=max(xmax,arg[0])\n",
    "        ymin=min(ymin,arg[1])\n",
    "        ymax=max(ymax,arg[1])\n",
    "#     plt.xlim(min(-1, xmin-1), max(1,xmax+1))\n",
    "#     plt.ylim(min(-1,ymin-1),max(1,ymax+1))\n",
    "\n",
    "def plot_contours(K,X=None, R=None):\n",
    "    '''This function plots the contours of a Bivariate Gaussian RV with\n",
    "    mean [0,0] and covariance K'''\n",
    "    \n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    y = np.linspace(-4, 4, 100)\n",
    "    xm, ym = np.meshgrid(x, np.flip(y))\n",
    "    if X is None:\n",
    "        X = np.dstack([xm,ym])\n",
    "    if R is not None:\n",
    "        X = X@R\n",
    "    \n",
    "    G = stats.multivariate_normal.pdf(X,mean=[0,0],cov=K)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.contour(xm,ym,G, extent=[-3,3,-3,3],cmap='viridis');\n",
    "    \n",
    "def makerot(theta):\n",
    "    '''This function creates a 2x2 rotation \n",
    "    matrix for a given angle (theta) in degrees'''\n",
    "    \n",
    "    theta=np.radians(theta)\n",
    "    \n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                  [np.sin(theta), np.cos(theta)]])\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde096f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats.multivariate_normal([0,0],[[1,0.8],[0.8,2]]).rvs(size=100)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6104bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1874594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contours\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen-decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d64e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort eigenvalues and eigenvectors in decreasing order of eigenvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc35650",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(K)\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotvec(V[:,0], V[:,1])\n",
    "plt.legend(['Data','Eigenvector 1', 'Eigenvector 2']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate data (uncorrelate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance of rotated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(K2)\n",
    "plt.scatter(rotated[0,:],rotated[1,:])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d355f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle between standard unit vector [1,0] and 1st eigenvector\n",
    "\n",
    "e1=np.array([[1],[0]])\n",
    "\n",
    "np.degrees(np.arccos((e1.T@V[:,0])/(np.linalg.norm(e1)*np.linalg.norm(V[:,0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the data by applying rotation matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance of resulting rotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(K3)\n",
    "plt.scatter(rotated2[0,:],rotated2[1,:])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef41e5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849508d",
   "metadata": {},
   "source": [
    "Coming back to the wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85feacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_train = ##\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_train)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(13),df_wine.columns[1:],rotation=90)\n",
    "plt.yticks(range(13),df_wine.columns[1:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23272f",
   "metadata": {},
   "source": [
    "Building a function to implement PCA from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA(X, m, display=1):\n",
    "    '''This function implements PCA. The data matrix X is DxN matrix, \n",
    "    where D is the dimension and N the number of points'''\n",
    "    \n",
    "    D, N = X.shape\n",
    "    \n",
    "    # Demean the Data\n",
    "    data = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Covariance of the input  X\n",
    "    cov_mat = np.cov(data)\n",
    "    \n",
    "    # Find eigenvectors and eigenvalues \n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(cov_mat)\n",
    "    \n",
    "    # Sort eigenvectors by magnitude of eigenvalues\n",
    "    L = eigen_vals[::-1]\n",
    "    U = eigen_vecs[:,::-1]\n",
    "\n",
    "    # Linear transformation\n",
    "    A = U[:,:m].T\n",
    "    \n",
    "    #compute explained variance and visualize it\n",
    "    cumulative_var_exp=0\n",
    "    total = sum(L)\n",
    "    var_explained = [(i/total) for i in L]\n",
    "    cumulative_var_exp = np.cumsum(var_explained)\n",
    "    \n",
    "    if display:\n",
    "        plt.bar(range(1,D+1), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "        plt.step(range(1,D+1), cumulative_var_exp, alpha=0.5, where='mid', label='cumulative explained variance')\n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.legend(loc='best');\n",
    "        \n",
    "    return A, var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5872de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86daccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f59d2",
   "metadata": {},
   "source": [
    "The resulting plot indicates that the first principal component alone accounts for 40 percent of the variance. Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the data.\n",
    "\n",
    "Although the explained variance plot reminds us of the feature importance, we shall remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = ##\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_mat)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31c96c",
   "metadata": {},
   "source": [
    "## PCA with ```scikit-learn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af350b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix A = U.T is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(1,14),np.cumsum(pca.explained_variance_ratio_),c='r')\n",
    "plt.bar(range(1,14),pca.explained_variance_ratio_, alpha=0.5)\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce7ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pca = ##\n",
    "\n",
    "y_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = ##\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(cov_mat)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(13),df_wine.columns[1:],rotation=90)\n",
    "plt.yticks(range(13),df_wine.columns[1:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(t_train), colors, markers):\n",
    "    plt.scatter(y_train_pca[t_train==l, 0], y_train_pca[t_train==l, 1],c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67633fd6",
   "metadata": {},
   "source": [
    "The training data is used to find the new features (eigenvectors). We can then represent the test set in this new feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b893de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pca = ##\n",
    "\n",
    "y_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa45f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b700c",
   "metadata": {},
   "source": [
    "## Example: Eigenfaces\n",
    "\n",
    "* Part of this example comes from: http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces = fetch_olivetti_faces(return_X_y=False)\n",
    "\n",
    "# print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces.data # data matrix\n",
    "\n",
    "t = faces.target # target label\n",
    "\n",
    "X.shape, t.shape # 400 images, each of size 64x64=4096 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7222c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(40):\n",
    "    fig.add_subplot(7,6,i+1)\n",
    "    idx = np.random.choice(np.where(t==i)[0])\n",
    "    plt.imshow(X[idx,:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b987323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train.shape, t_train.shape, X_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ##\n",
    "pca.fit_transform(X_train_scaled);\n",
    "\n",
    "plt.plot(100*np.cumsum(pca.explained_variance_ratio_), '-o')\n",
    "plt.xlabel('Principal Components',size=15)\n",
    "plt.ylabel('Cumulative Explained Variance, in %', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ab779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d16d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "359a07cf",
   "metadata": {},
   "source": [
    "In order to explain 90% of the variance in the data, we need to preserve 63 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3099ffa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec374980",
   "metadata": {},
   "source": [
    "Let's project to 2-D so we can plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "ypca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "ypca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(ypca[:,0], ypca[:,1], c=t_train, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('Principal Component 1', size=15)\n",
    "plt.ylabel('Principal Component 2', size=15)\n",
    "plt.title('Training Set')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263883d",
   "metadata": {},
   "source": [
    "Not that the 40 classes are overlapping in the linear projection space. This is because PCA is **unsupervised**, it does use the class labels *anywhere* in finding the matrix for linear projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe47ced",
   "metadata": {},
   "source": [
    "To apply this transformation in the test set, simply multiply the resultant modal matrix with the scaled test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set using the linear transformation found with the training data\n",
    "ypca_test = ##\n",
    "\n",
    "ypca_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ac700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(ypca_test[:,0], ypca_test[:,1], c=t_test, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('Principal Component 1', size=15)\n",
    "plt.ylabel('Principal Component 2', size=15)\n",
    "plt.title('Test Set')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8dddc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893013d",
   "metadata": {},
   "source": [
    "You can access the linear transformation $\\mathbf{A} = \\mathbf{U}^T$ using the method ```components_```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ##\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157caf79",
   "metadata": {},
   "source": [
    "Note that the eigenvectors are described in the original space, that is, they are 4096-dimensional!\n",
    "\n",
    "Since we are working with images, we can reshape them back to a $64 \\times 64$ image and see what are the regions in the image with maximum explained variance! This is called the **eigenfaces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = ##\n",
    "\n",
    "yyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(yyy[:,0], yyy[:,1], c=t_test, cmap=plt.cm.gist_rainbow)\n",
    "plt.xlabel('Principal Component 1', size=15)\n",
    "plt.ylabel('Principal Component 2', size=15)\n",
    "plt.title('Test Set')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2886962",
   "metadata": {},
   "source": [
    "What do eigenvectors represent in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926acf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 1st eigenvector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e9b49",
   "metadata": {},
   "source": [
    "Let's now recover 16 eigenvectors and plot them as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = ##\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "ypca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(n_components):\n",
    "    fig.add_subplot(4,4,i+1)\n",
    "    plt.imshow(abs(pca.components_[i,:].reshape(64,64)),cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8dc61",
   "metadata": {},
   "source": [
    "The eigenvectors are describing the regions in the 64x64 image that explain the most variance. the more eigenvectors are kept, the better a reconstruction image will be produced.\n",
    "\n",
    "For example, let's reconstruct the images in the dataset using the top 16 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = ##\n",
    "\n",
    "L, U = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8c892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eigenvectors = ##\n",
    "\n",
    "P = U[:, :N_eigenvectors]\n",
    "\n",
    "X_proj = X_train_scaled@P\n",
    "X_reconstruct = X_proj@np.linalg.pinv(P)\n",
    "\n",
    "X_reconstruct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434258af",
   "metadata": {},
   "source": [
    "Since the projection is given by:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{A}\\mathbf{X}$$\n",
    "\n",
    "In order to recover $\\mathbf{X}$, we need to left-multiply by the pseudo-inverse of $\\mathbf{A}$:\n",
    "\n",
    "$$\\hat{\\mathbf{X}} = \\mathbf{A}^\\dagger\\mathbf{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a44da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "\n",
    "ypca = pca.transform(X_train_scaled)\n",
    "X_reconstruct_skl=pca.inverse_transform(ypca)\n",
    "\n",
    "X_reconstruct_skl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5401690",
   "metadata": {},
   "source": [
    "We also need to bringing back to the original scaling: multiplying by the standard deviation and adding the sample mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba76b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed = scaler.inverse_transform(X_reconstruct)\n",
    "\n",
    "X_reconstructed_skl = scaler.inverse_transform(X_reconstruct_skl)\n",
    "\n",
    "# np.multiply(X_reconstruct,stdev) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "idx = np.random.choice(range(X_reconstructed.shape[0]),replace=False,size=N)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "j=1\n",
    "for i in range(N):\n",
    "    fig.add_subplot(2,N,j)\n",
    "    plt.imshow(X_train[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image');\n",
    "\n",
    "    fig.add_subplot(2,N,j+N)\n",
    "    plt.imshow(X_reconstructed[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Reconstructed Image');\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18964cf6",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eigenvectors = ##\n",
    "\n",
    "pca = PCA(n_components=N_eigenvectors)\n",
    "ypca = pca.fit_transform(X_train_scaled)\n",
    "X_reconstruct = pca.inverse_transform(ypca)\n",
    "X_reconstructed = scaler.inverse_transform(X_reconstruct)\n",
    "\n",
    "N = 5\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "idx = np.random.choice(range(X_reconstructed.shape[0]),replace=False,size=N)\n",
    "j=1\n",
    "for i in range(N):\n",
    "    fig.add_subplot(2,N,j)\n",
    "    plt.imshow(X_train[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image');\n",
    "\n",
    "    fig.add_subplot(2,N,j+N)\n",
    "    plt.imshow(X_reconstructed[idx[i],:].reshape(64,64), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Reconstructed Image');\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011844e",
   "metadata": {},
   "source": [
    "Still some compression loss but much better representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe6618",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
