{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: GMM as Density Estimation\n",
    "\n",
    "Consider some data generated from Scikit-Learn's ```make_moons``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n",
    "plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the GMM output. \n",
    "\n",
    "* Let's not worry about the implementation of this helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from \"Python Data Science Handbook\"\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to fit this with a two-component GMM viewed as a clustering model, the results are not particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "GaussianMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM = GaussianMixture(n_components=2, covariance_type='full', random_state=0).fit(Xmoon)\n",
    "plot_gmm(GMM, Xmoon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we instead use many more components and ignore the cluster labels, we find a fit that is much closer to the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM16 = GaussianMixture(n_components=16, random_state=0).fit(Xmoon)\n",
    "plot_gmm(GMM16, Xmoon, label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall distribution of the input data. This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here are 400 new points drawn from this 16-component GMM fit to our original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = GMM16.sample(100)\n",
    "plt.scatter(Xnew[0][:, 0], Xnew[0][:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM is convenient as a flexible means of modeling an arbitrary multi-dimensional distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-5,10,1000)\n",
    "\n",
    "mus = [-2, 0, 1.5, 3, 5]\n",
    "sigs_sq = [0.1, 1, 1, 2, 0.5]\n",
    "Pis = [.2, 0.15, .25, .25, .15]\n",
    "\n",
    "symbols=['-b','--r','-.g','--m','orange']\n",
    "\n",
    "mixture_density=0\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(len(mus)):\n",
    "    plt.plot(x, stats.norm(loc=mus[i], scale=np.sqrt(sigs_sq[i])).pdf(x),\n",
    "             symbols[i],\n",
    "             label='Gaussian($\\mu='+str(mus[i])+',\\sigma^2='+str(sigs_sq[i])+'$)')\n",
    "    mixture_density += Pis[i]*stats.norm(loc=mus[i], scale=np.sqrt(sigs_sq[i])).pdf(x)\n",
    "plt.legend(fontsize=15)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, mixture_density, label='Gaussian Mixture')\n",
    "plt.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uni_GaussianMixture(N, mus, sigs_sq, Pis):\n",
    "    X = np.empty(0)\n",
    "    L = np.empty(0)\n",
    "    for i in range(N):\n",
    "        rv = np.random.uniform()   # sample uniform RV\n",
    "        GaussianChosen = np.where(rv < np.cumsum(Pis))[0][0] # This selects the Gaussian label\n",
    "        L = np.append(L, GaussianChosen)\n",
    "        X = np.append(X, stats.norm(loc=mus[GaussianChosen], scale=np.sqrt(sigs_sq[GaussianChosen])).rvs(size=1))\n",
    "    return X[:,np.newaxis], L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10_000\n",
    "mus = [-2, 0, 1.5, 3, 5]\n",
    "sigs_sq = [0.1, 1, 1, 2, 0.5]\n",
    "Pis = [.2, 0.15, .25, .25, .15]\n",
    "colors=['purple','blue','green','yellow','orange']\n",
    "\n",
    "data, L = make_uni_GaussianMixture(N, mus, sigs_sq, Pis)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "for i in range(len(mus)):\n",
    "    plt.scatter(data[L==i], i*np.ones(len(data[L==i])), c=colors[i])\n",
    "plt.title('Synthetic Data with known Gaussians')\n",
    "plt.subplot(2,2,3)\n",
    "for i in range(len(mus)):\n",
    "    plt.scatter(data[L==i], np.ones(len(data[L==i])), c=colors[i], alpha=0.3)\n",
    "plt.title('Synthetic Data as Received from the World')\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(data, density=True, label='Synthetic Data Histogram')\n",
    "plt.plot(x, mixture_density, label='(True) Gaussian Mixture')\n",
    "plt.legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Function to Train and Plot a GMM Density Estimation with the EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_predict_plot(X,T,Nclusters,d=1,init_parms='kmeans',covariance_type='full',plotting=True):\n",
    "    if init_parms == 'random':\n",
    "        # select points at random from dataset\n",
    "        N, D = X.shape\n",
    "        index_loc=np.random.randint(0,N,size=Nclusters)\n",
    "        means_init = X[index_loc,:]\n",
    "        GMM = GaussianMixture(n_components=Nclusters,\n",
    "                          covariance_type=covariance_type,\n",
    "                              init_params=init_parms,\n",
    "                             means_init=means_init).fit(X)\n",
    "    else:\n",
    "        GMM = GaussianMixture(n_components=Nclusters,\n",
    "                              covariance_type=covariance_type,\n",
    "                              init_params=init_parms).fit(X)\n",
    "    labels = GMM.predict(X)\n",
    "    prob = GMM.predict_proba(X).round(2)\n",
    "    # Estimated parameters\n",
    "    Means=GMM.means_\n",
    "    Sigmas=GMM.covariances_\n",
    "    Pis=GMM.weights_\n",
    "\n",
    "    if plotting:\n",
    "        if d==2:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.scatter(X[:,0],X[:,1], c=T)\n",
    "            plt.title('True (Unknown) Labels');\n",
    "\n",
    "            fig = plt.figure(figsize=(15,10))\n",
    "            fig.add_subplot(2,Nclusters,1)\n",
    "            plt.scatter(X[:,0],X[:,1], c=labels)\n",
    "            plt.title('Predicted Labels by GMM');\n",
    "            for i in range(Nclusters):\n",
    "                ax = fig.add_subplot(2,Nclusters,Nclusters+i+1)\n",
    "                p1 = ax.scatter(X[:,0], X[:,1], c=prob[:,i])\n",
    "                fig.colorbar(p1, ax=ax)\n",
    "                plt.title('Memberships\\n Cluster '+str(i+1))\n",
    "        else: #d===1\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.scatter(X, np.ones(len(X)), c=T)\n",
    "            plt.title('True (Unknown) Labels');\n",
    "\n",
    "            fig = plt.figure(figsize=(15,10))\n",
    "            fig.add_subplot(2,Nclusters,1)\n",
    "            plt.scatter(X, np.ones(len(X)), c=labels)\n",
    "            plt.title('Predicted Labels by GMM');\n",
    "            for i in range(Nclusters):\n",
    "                ax = fig.add_subplot(2,Nclusters,Nclusters+i+1)\n",
    "                p1 = ax.scatter(X, np.ones(len(X)), c=prob[:,i])\n",
    "                fig.colorbar(p1, ax=ax)\n",
    "                plt.title('Memberships\\n Cluster '+str(i+1))\n",
    "            \n",
    "    return Means, Sigmas, Pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "\n",
    "Means, Sigmas, Pis=GMM_predict_plot(data,L,n_components, d=1,\n",
    "                                    init_parms='random',\n",
    "                                    covariance_type='full',\n",
    "                                    plotting=False);\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "x=np.linspace(-5,10,1000)\n",
    "estimated_mixture=0\n",
    "for i in range(len(Pis)):\n",
    "    estimated_mixture += Pis[i]*stats.norm(loc=Means[i], scale=np.sqrt(Sigmas[i][0])).pdf(x)\n",
    "plt.plot(x, estimated_mixture, 'r', label='Predicted')\n",
    "plt.plot(x, mixture_density, '--g', label='True Mixture Model')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: GMM as a Clustering Algorithm\n",
    "\n",
    "GMM is commonly used as an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.\n",
    "\n",
    "However, a common practical use for the GMM is as a clustering algorithm, where we are interested in finding groups in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this, I will use the ```scikit-learn``` algorithm implementation of the GMM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Create synthetically generate data\n",
    "n_samples = 1500\n",
    "X1, T1 = datasets.make_blobs(n_samples=n_samples,centers=3,cluster_std=1)               # Blobs data\n",
    "X2, T2 = datasets.make_blobs(n_samples=n_samples,cluster_std=[1.0, 2.5, 0.5],centers=3) # Different Variance Blobs data\n",
    "X3, T3 = datasets.make_moons(n_samples=n_samples, noise=.05)                            # Moons data\n",
    "X4, T4 = datasets.make_circles(n_samples, noise=.05, factor=0.5)                        # Circles data\n",
    "X5     = np.dot(X1, [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]])             # Anisotropicly distributed data\n",
    "T5     = T1\n",
    "X6     = np.vstack((X1[T1 == 0][:500], X1[T1 == 1][:100], X1[T1 == 2][:10]))            # Unevenly sized Blobs data\n",
    "T6     = np.hstack((np.zeros(500),np.ones(100),2*np.ones(10))).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,6,1); plt.scatter(X1[:,0],X1[:,1],c=T1)\n",
    "plt.subplot(1,6,2); plt.scatter(X2[:,0],X2[:,1],c=T2)\n",
    "plt.subplot(1,6,3); plt.scatter(X3[:,0],X3[:,1],c=T3)\n",
    "plt.subplot(1,6,4); plt.scatter(X4[:,0],X4[:,1],c=T4)\n",
    "plt.subplot(1,6,5); plt.scatter(X5[:,0],X5[:,1],c=T5)\n",
    "plt.subplot(1,6,6); plt.scatter(X6[:,0],X6[:,1],c=T6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "def GMM_predict_plot(X,T,Nclusters,init_parms='kmeans',covariance_type='full'):\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    # select mean values as random samples from dataset X\n",
    "    if init_parms == 'random':\n",
    "        index_loc=np.random.randint(0,N,size=Nclusters)\n",
    "        means_init = X[index_loc,:]\n",
    "        \n",
    "        GMM = GaussianMixture(n_components=Nclusters,\n",
    "                          covariance_type=covariance_type,\n",
    "                              init_params=init_parms,\n",
    "                             means_init=means_init).fit(X)\n",
    "    \n",
    "    # otherwise, initialize with k-means clustering solution\n",
    "    else:\n",
    "        GMM = GaussianMixture(n_components=Nclusters,\n",
    "                              covariance_type=covariance_type,\n",
    "                              init_params=init_parms).fit(X)\n",
    "    labels = GMM.predict(X)\n",
    "    prob = GMM.predict_proba(X).round(2)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.grid(True)\n",
    "    plt.scatter(X[:,0],X[:,1], c=T)\n",
    "    plt.title('True (Unknown) Labels');\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    fig.add_subplot(2,Nclusters,1)\n",
    "    plt.scatter(X[:,0],X[:,1], c=labels)\n",
    "    plt.grid(True)\n",
    "    plt.title('Predicted Labels by GMM');\n",
    "    for i in range(Nclusters):\n",
    "        ax = fig.add_subplot(2,Nclusters,Nclusters+i+1)\n",
    "        p1 = ax.scatter(X[:,0], X[:,1], c=prob[:,i])\n",
    "        fig.colorbar(p1, ax=ax)\n",
    "        plt.title('Memberships in Cluster '+str(i+1))\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GMM_predict_plot(X1,T1,3,'random','diag');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_predict_plot(X2,T2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GMM_predict_plot(X3,T3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_predict_plot(X4,T4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GMM_predict_plot(X5,T5,3,'random','diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GMM_predict_plot(X6,T6,3,'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualizations with Code from Scratch\n",
    "\n",
    "(This code will be shared after the solutions of upcoming assignment are posted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Code (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Interactice Code on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=np.array(['magenta','orange','blue','green','red','cyan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NumComponents = 3\n",
    "plt.scatter(X1[:,0],X1[:,1],c=colors[T1])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X1, NumComponents,50,0.1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumComponents = 3\n",
    "plt.scatter(X2[:,0],X2[:,1],c=colors[T2])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X2, NumComponents,50,0.1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumComponents = 2\n",
    "plt.scatter(X3[:,0],X3[:,1],c=colors[T3])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X3, NumComponents,50,0.1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumComponents = 2\n",
    "plt.scatter(X4[:,0],X4[:,1],c=colors[T4])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X4, NumComponents,50,0.1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NumComponents = 3\n",
    "plt.scatter(X5[:,0],X5[:,1],c=colors[T5])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X5, NumComponents,50,0.1)\n",
    "# we are assuming the covariance is a full covariance (not isotropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NumComponents = 3\n",
    "plt.scatter(X6[:,0],X6[:,1],c=colors[T6])\n",
    "plt.title('Original Data',size=15);plt.show()\n",
    "\n",
    "EM_Means, EM_Sigs, EM_Ps, pZ_X = EM_GaussianMixture(X6, NumComponents,50,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
