{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9 Part 2 - Introduction to Mixture Models & Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the data for a *single class* looks like the plot below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples = 1500, centers = 5)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1]); plt.axis([-15,15,-15,15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a single Gaussian distribution, we would obtain a very poor estimate of the true underlying data likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models\n",
    "\n",
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align*}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "A **Gaussian Mixture Model** or **GMM** is a probabilistic model that assumes a data likelihood to be a weighted sum of Gaussian distributions with unknown parameters.\n",
    "\n",
    "$$p(\\mathbf{x}|\\Theta) = \\sum_{k=1}^K \\pi_k N(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)$$\n",
    "\n",
    "where $\\Theta=\\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$, $0 \\leq \\pi_k \\leq 1$ and $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "* When standard distributions (such as Gamma, Exponential, Gaussian, etc.) are not sufficient to characterize a *complicated* data likelihood, we can instead characterize it as the sum of weighted Gaussians distributions\n",
    "\n",
    "* Another way that GMMs are most commonly used for is to partition data in subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a Data Likelihood as a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMMs can be used to learn a complex distribution that represent a dataset. Thus, it can be used within the probabilistic generative classifier framework to model complex data likelihoods.\n",
    "\n",
    "* GMMs are also commonly used for **clustering**. Here a GMM is fit to a dataset with the goal of partitioning it into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 1</font>**\n",
    "\n",
    "Describe the **observed data likelihood**, $\\mathcal{L}^o$. As seen last class:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 2</font>**\n",
    "\n",
    "Describe the log-likelihood function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\ln\\left(\\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\\right)\\\\\n",
    "\\iff \\mathcal{L} &= \\sum_{i=1}^N \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 3</font>**\n",
    "\n",
    "Optimize for the parameters $\\Theta=\\{\\pi_k, \\mu_k,\\Sigma_k\\}_{k=1}^K$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} = 0, \\text{ and }, \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = 0\n",
    "\\end{align*}\n",
    "\n",
    "but this is a difficult problem to maximize!\n",
    "\n",
    "* A common approach for estimating the parameters of a GMM given a data set is by using the **Expectation-Maximization (EM) algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Mixture Models\n",
    "\n",
    "* **Data Representation and Inference**. Represent *any* data set using a complex distribution. You may want to describe your data with a distribution but the data does not fit into a standard form (e.g. Gamma, Gaussian, Exponential, etc.), then you can use a (Gaussian) Mixture Model to describe the data. Having a representation of your data in a distribution form is a very powerful tool that, other than having a model of the data, allows you infer about new samples, make predictions, etc.\n",
    "    * Having a parametric form for a real world model, allows us to apply it in simulation and use it for designing/optimizing decision-making solutions.\n",
    "\n",
    "* **Clustering.** Partition the data into groups. Note that in the GMMs formulation we did not add the concept of labels/targets. So GMMs are an **unsupervised learning** model. It represents the data with a very complex likelihood and then we can decompose that likelihood to partition the data into categories. This is also known as **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
